{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8/26/2020-Data-Prep.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN9sfmD/f8a2hGwRv7r5vW4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khickey25/nlp_shap_notebooks/blob/master/8_26_2020_Data_Prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8QEqbaeuKI4",
        "colab_type": "text"
      },
      "source": [
        "# Data Prep for Fake news\n",
        "\n",
        "I needed to fix the text before it would be processed and saved to the pickle file, so I do that here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqtEa9RXI4-N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "4d90b8c6-ab16-4a78-c3f7-51fa0b70b2c5"
      },
      "source": [
        "\n",
        "\n",
        "#housekeeping steps\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install shap\n",
        "\n",
        "import pandas as pd\n",
        "%tensorflow_version 1.13.1\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import text\n",
        "import keras.backend.tensorflow_backend as K\n",
        "K.set_session\n",
        "import matplotlib.pyplot as plt\n",
        "import shap\n",
        "import sklearn.metrics as skm\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pprint\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.6/dist-packages (0.35.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from shap) (1.0.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from shap) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from shap) (1.18.5)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.6/dist-packages (from shap) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from shap) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->shap) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->shap) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->shap) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->shap) (1.15.0)\n",
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.13.1`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLEa6Be7LL0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_file = \"/content/drive/My Drive/test_data/fake_news_20200823.csv\"\n",
        "pickle_file = \"/content/drive/My Drive/test_data/fake-news-20200824-v2.pickle\"\n",
        "full_df = pd.read_csv(csv_file).reset_index(drop=True)\n",
        "DF = pd.read_csv(csv_file)\n",
        "with open(pickle_file, 'rb') as pickle_file:\n",
        "  text_train = pickle.load(pickle_file)\n",
        "  y_train = pickle.load(pickle_file)\n",
        "  text_test = pickle.load(pickle_file)\n",
        "  y_test = pickle.load(pickle_file)\n",
        "for df in [text_train, y_train, text_test, y_test]:\n",
        "  df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "#scratch debugging info: not info here\n",
        "a,b,c,d =  train_test_split(full_df.text, full_df.label, test_size=0.2, \n",
        "                                                    random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b_R3TT_wYD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hacky way to remove reuters from original text and \n",
        "#update the content column so that it does not have any\n",
        "#\"(Reuters) -\" in there. \n",
        "#fake articles tend to have a sourcing sentence at end of article,\n",
        "#so use crude implementation to remove last 40 characters of text\n",
        "#takes about 90sec to run\n",
        "for i, text in enumerate(DF['text']):\n",
        "    if '(Reuters) -' in text:\n",
        "      c = text[re.search(\"\\) -\", text).end()+1:]\n",
        "      DF.loc[i,'text'] = c\n",
        "    else:\n",
        "      DF.loc[i, 'text'] = text[:-40]\n",
        "\n",
        "\n",
        "DF['content'] = DF['title'] + DF['text']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flmhFRfiz7xf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "a1f4af2d-6a39-4137-ed41-1d6e0b68f1a8"
      },
      "source": [
        "DF.iloc[0]['content']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'No. 2 Senate Republican: health bill to be discussed Tuesday before voteSenator John Cornyn, the No. 2 Senate Republican, said on Monday that various options for a healthcare bill will be discussed at a policy lunch on Tuesday prior to the first procedural vote. Republican Senator Orrin Hatch said he did not expect Senator John McCain, who is recuperating from surgery in his home state of Arizona, to return to Washington in time for Tuesdayâ€™s healthcare vote. '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YApgl5ggrf01",
        "colab_type": "text"
      },
      "source": [
        "# Data Prep\n",
        "\n",
        "Same preprocessing steps as done in Tao's notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ_rGRcQmVw7",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fb875e04-619c-440c-fda6-16ea78985a01"
      },
      "source": [
        "#@title\n",
        "#code for TODO 8/26/12:28\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import re\n",
        "import sys\n",
        "import warnings\n",
        "#data = sent_df1\n",
        "if not sys.warnoptions:\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "def cleanHtml(sentence):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, ' ', str(sentence))\n",
        "    return cleantext\n",
        "def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
        "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
        "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
        "    cleaned = cleaned.strip()\n",
        "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
        "    return cleaned\n",
        "def keepAlpha(sentence):\n",
        "    alpha_sent = \"\"\n",
        "    for word in sentence.split():\n",
        "        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n",
        "        alpha_sent += alpha_word\n",
        "        alpha_sent += \" \"\n",
        "    alpha_sent = alpha_sent.strip()\n",
        "    return alpha_sent\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update(['zero','one','two','three','four','five','six',\n",
        "                   'seven','eight','nine','ten','may','also','across','among','beside',\n",
        "                   'however','yet','within', 'almost'])\n",
        "re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\n",
        "def removeStopWords(sentence):\n",
        "    global re_stop_words\n",
        "    return re_stop_words.sub(\" \", sentence)\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "def stemming(sentence):\n",
        "    stemSentence = \"\"\n",
        "    for word in sentence.split():\n",
        "        stem = stemmer.stem(word)\n",
        "        stemSentence += stem\n",
        "        stemSentence += \" \"\n",
        "    stemSentence = stemSentence.strip()\n",
        "    return stemSentence\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R80O2-e0oIlQ",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "\n",
        "review_text = DF.content.str.lower()\n",
        "\n",
        "review_text = review_text.apply(cleanHtml)\n",
        "review_text = review_text.apply(cleanPunc)\n",
        "review_text = review_text.apply(keepAlpha)\n",
        "\n",
        "review_text = review_text.apply(removeStopWords)\n",
        "review_text = review_text.apply(stemming)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNuoXrUsmVuq",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "from sklearn.model_selection import train_test_split\n",
        "text_train, text_test, y_train, y_test = train_test_split(review_text, DF.label, test_size=0.2, \n",
        "                                                    random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq9TH4xqmVrQ",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "import pickle\n",
        "\n",
        "#saving it to my google drive\n",
        "file_path = '/content/drive/My Drive/test_data/fake-news-20200826-cleaned.pickle'\n",
        "with open(file_path, 'wb') as pickle_file:\n",
        "    pickle.dump(text_train, pickle_file)\n",
        "    pickle.dump(y_train, pickle_file)\n",
        "    pickle.dump(text_test, pickle_file)\n",
        "    pickle.dump(y_test, pickle_file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}